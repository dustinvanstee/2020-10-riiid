{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017394,
     "end_time": "2020-11-10T14:29:56.748410",
     "exception": false,
     "start_time": "2020-11-10T14:29:56.731016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The goal of this notebook is to build a slightly sophisticated dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:29:57.842647Z",
     "start_time": "2020-11-17T03:29:53.160235Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 1.266023,
     "end_time": "2020-11-10T14:29:58.030905",
     "exception": false,
     "start_time": "2020-11-10T14:29:56.764882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import riiideducation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "import copy\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import dataframe as dd \n",
    "\n",
    "\n",
    "\n",
    "def npt(mystring) :\n",
    "    print(\"**{}** : {}\".format(sys._getframe(1).f_code.co_name,mystring))\n",
    "    \n",
    "#env = riiideducation.make_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01848,
     "end_time": "2020-11-10T14:29:58.066319",
     "exception": false,
     "start_time": "2020-11-10T14:29:58.047839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In a previous notebook, I read in and then pickled the data.  Lets see how fast i can load it.  Note, you need to add the other notebook to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:30:10.638277Z",
     "start_time": "2020-11-17T03:30:08.662124Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 43.890352,
     "end_time": "2020-11-10T14:30:41.973624",
     "exception": false,
     "start_time": "2020-11-10T14:29:58.083272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 99271300 entries, 0 to 99271299\n",
      "Data columns (total 8 columns):\n",
      " #   Column                          Dtype  \n",
      "---  ------                          -----  \n",
      " 0   timestamp                       int64  \n",
      " 1   user_id                         int32  \n",
      " 2   content_id                      int16  \n",
      " 3   content_type_id                 int8   \n",
      " 4   task_container_id               int16  \n",
      " 5   answered_correctly              int8   \n",
      " 6   prior_question_elapsed_time     float32\n",
      " 7   prior_question_had_explanation  boolean\n",
      "dtypes: boolean(1), float32(1), int16(2), int32(1), int64(1), int8(2)\n",
      "memory usage: 3.0 GB\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_pickle('full_df.pkl')\n",
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>5692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1805962620</td>\n",
       "      <td>5547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2015251289</td>\n",
       "      <td>4024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>867941388</td>\n",
       "      <td>6659</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>867946278</td>\n",
       "      <td>3977</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99271295</th>\n",
       "      <td>87193076570</td>\n",
       "      <td>626308830</td>\n",
       "      <td>8185</td>\n",
       "      <td>0</td>\n",
       "      <td>9217</td>\n",
       "      <td>0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99271296</th>\n",
       "      <td>87193279051</td>\n",
       "      <td>626308830</td>\n",
       "      <td>6686</td>\n",
       "      <td>0</td>\n",
       "      <td>9218</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99271297</th>\n",
       "      <td>87193332075</td>\n",
       "      <td>626308830</td>\n",
       "      <td>5860</td>\n",
       "      <td>0</td>\n",
       "      <td>9219</td>\n",
       "      <td>1</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99271298</th>\n",
       "      <td>87193355096</td>\n",
       "      <td>626308830</td>\n",
       "      <td>11465</td>\n",
       "      <td>0</td>\n",
       "      <td>9220</td>\n",
       "      <td>0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99271299</th>\n",
       "      <td>87425772049</td>\n",
       "      <td>705741139</td>\n",
       "      <td>12040</td>\n",
       "      <td>0</td>\n",
       "      <td>3394</td>\n",
       "      <td>1</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99271300 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp     user_id  content_id  content_type_id  \\\n",
       "0                   0         115        5692                0   \n",
       "1                   0  1805962620        5547                0   \n",
       "2                   0  2015251289        4024                0   \n",
       "3                   0   867941388        6659                0   \n",
       "4                   0   867946278        3977                0   \n",
       "...               ...         ...         ...              ...   \n",
       "99271295  87193076570   626308830        8185                0   \n",
       "99271296  87193279051   626308830        6686                0   \n",
       "99271297  87193332075   626308830        5860                0   \n",
       "99271298  87193355096   626308830       11465                0   \n",
       "99271299  87425772049   705741139       12040                0   \n",
       "\n",
       "          task_container_id  answered_correctly  prior_question_elapsed_time  \\\n",
       "0                         1                   1                          NaN   \n",
       "1                         0                   0                          NaN   \n",
       "2                         0                   1                          NaN   \n",
       "3                         0                   1                          NaN   \n",
       "4                         0                   1                          NaN   \n",
       "...                     ...                 ...                          ...   \n",
       "99271295               9217                   0                      13000.0   \n",
       "99271296               9218                   0                      10000.0   \n",
       "99271297               9219                   1                      21000.0   \n",
       "99271298               9220                   0                      25000.0   \n",
       "99271299               3394                   1                      16000.0   \n",
       "\n",
       "          prior_question_had_explanation  \n",
       "0                                   <NA>  \n",
       "1                                   <NA>  \n",
       "2                                   <NA>  \n",
       "3                                   <NA>  \n",
       "4                                   <NA>  \n",
       "...                                  ...  \n",
       "99271295                            True  \n",
       "99271296                            True  \n",
       "99271297                            True  \n",
       "99271298                            True  \n",
       "99271299                            True  \n",
       "\n",
       "[99271300 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Question averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016712,
     "end_time": "2020-11-10T14:30:42.008948",
     "exception": false,
     "start_time": "2020-11-10T14:30:41.992236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets do a simple calculation to find average pct correct per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:30:53.107921Z",
     "start_time": "2020-11-17T03:30:42.940326Z"
    },
    "papermill": {
     "duration": 22.570852,
     "end_time": "2020-11-10T14:31:04.596855",
     "exception": false,
     "start_time": "2020-11-10T14:30:42.026003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question_averages_df = full_df[['content_id','answered_correctly']].groupby(['content_id']).agg(['sum','count'])\n",
    "question_averages_df.columns = ['correct','total']\n",
    "question_averages_df[\"pct_correct\"] = question_averages_df['correct'] / question_averages_df['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:30:57.561996Z",
     "start_time": "2020-11-17T03:30:57.532205Z"
    },
    "papermill": {
     "duration": 0.056809,
     "end_time": "2020-11-10T14:31:04.671414",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.614605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_averages_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/s4s004/vanstee/anaconda3/envs/oce-pytorch-1.6/lib/python3.6/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 32903 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=40,\n",
    "    threads_per_worker=1,\n",
    "    processes=True,memory_limit=10e9)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46716</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:32903/status' target='_blank'>http://127.0.0.1:32903/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>40</li>\n",
       "  <li><b>Cores: </b>40</li>\n",
       "  <li><b>Memory: </b>400.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46716' processes=40 threads=40, memory=400.00 GB>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recycle Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recycle_cluster(client, cluster) :\n",
    "    if cluster == None :\n",
    "        cluster = LocalCluster(n_workers=40,\n",
    "            threads_per_worker=1,\n",
    "            processes=True,memory_limit=10e9)\n",
    "    if(client == None) :\n",
    "        client = Client(cluster)\n",
    "    else :\n",
    "        client.close()\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = recycle_cluster(client, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ddf = dd.from_pandas(full_df, npartitions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Question  Averages to full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def add_question_averages() :\n",
    "#     question_averages_ddf = full_ddf[['content_id','answered_correctly']].groupby(['content_id']).agg(['sum','count'])\n",
    "#     question_averages_ddf.columns = ['correct','total']\n",
    "# question_averages_ddf[\"pct_correct\"] = question_averages_ddf['correct'] / question_averages_ddf['total']\n",
    "# question_averages_ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10X Speedup for DASK !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df.iloc[0:1000,:].merge(question_averages_df,how=\"inner\",left_on=\"content_id\",right_on=\"content_id\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample + Train_Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T03:31:44.021292Z",
     "start_time": "2020-11-17T03:31:44.018442Z"
    }
   },
   "outputs": [],
   "source": [
    "def downsample_and_split(df) :\n",
    "    #num_samples = len(full_df)\n",
    "    #split_point = int(0.85*num_samples)\n",
    "    #train_df = full_df.iloc[0:split_point]\n",
    "    #val_df = full_df.iloc[split_point:num_samples]\n",
    "    # val_df.head(), timestamp = 17010520340\n",
    "    #num_samples = len(full_ddf)\n",
    "    #split_point = int(0.85*num_samples)\n",
    "    train_ddf = df[df[\"timestamp\"]< 17010520340]\n",
    "    val_ddf = df[df[\"timestamp\"]> 17010520340]\n",
    "    return train_ddf,val_ddf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x) :\n",
    "    return (1.0 / (1.0+ np.exp(-1.0*x)))\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users > N Questions Answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users > than 2 is 393146\n",
      "users > than 4 is 392323\n",
      "users > than 8 is 389104\n",
      "users > than 16 is 362047\n",
      "users > than 32 is 218884\n",
      "users > than 64 is 147230\n",
      "users > than 128 is 102072\n",
      "users > than 256 is 66486\n",
      "users > than 512 is 38255\n",
      "users > than 1024 is 18443\n",
      "users > than 2048 is 7031\n",
      "users > than 4096 is 1762\n",
      "users > than 8192 is 254\n",
      "users > than 16384 is 2\n",
      "users > than 32768 is 0\n"
     ]
    }
   ],
   "source": [
    "train_ddf,val_ddf = downsample_and_split(full_ddf)\n",
    "\n",
    "user_q_count = train_ddf.groupby('user_id').agg({\"user_id\":\"count\"})\n",
    "user_q_count.columns = [\"count\"]\n",
    "user_q_count.columns\n",
    "user_q_count = user_q_count.persist()\n",
    "for i in range(1,16) :\n",
    "    print(\"users > than {} is {}\".format( np.power(2,i),  len(user_q_count[user_q_count[\"count\"] > np.power(2,i)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19010\n"
     ]
    }
   ],
   "source": [
    "prolific_users = user_q_count[user_q_count[\"count\"] > 1000].compute().index.values.flatten().tolist()\n",
    "print(len(prolific_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about 10% of users are answering greater than 500 Q/A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ddf[~train_ddf[\"user_id\"].isin(prolific_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Cartesian Join Experiment!\n",
    "\n",
    "So the goal is that by user_id i want to create this table ..\n",
    "\n",
    "1. partial cartesian join on user_id\n",
    "user_id, content_id, answered_correctly, timestamp, user_id_p, content_id_p, answered_correctly_p, timestamp_p\n",
    "\n",
    "2.  then filter by timestamp > timestamp_p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42021035 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>5692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1805962620</td>\n",
       "      <td>5547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2015251289</td>\n",
       "      <td>4024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>867946278</td>\n",
       "      <td>3977</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>867947333</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp     user_id  content_id  content_type_id  task_container_id  \\\n",
       "0          0         115        5692                0                  1   \n",
       "1          0  1805962620        5547                0                  0   \n",
       "2          0  2015251289        4024                0                  0   \n",
       "4          0   867946278        3977                0                  0   \n",
       "5          0   867947333        7900                0                  0   \n",
       "\n",
       "   answered_correctly  prior_question_elapsed_time  \\\n",
       "0                   1                          NaN   \n",
       "1                   0                          NaN   \n",
       "2                   1                          NaN   \n",
       "4                   1                          NaN   \n",
       "5                   1                          NaN   \n",
       "\n",
       "   prior_question_had_explanation  \n",
       "0                            <NA>  \n",
       "1                            <NA>  \n",
       "2                            <NA>  \n",
       "4                            <NA>  \n",
       "5                            <NA>  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get non-prolific users ..\n",
    "t_ddf = train_ddf[~train_ddf[\"user_id\"].isin(prolific_users)]\n",
    "t_ddf = t_ddf.persist()\n",
    "print(len(t_ddf),t_ddf.npartitions)\n",
    "t_ddf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large table join .. use index join .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.scale(40, memory=10e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large hash join using columns .. avoid... \n",
    "# cols = ['user_id', 'content_id', 'answered_correctly','timestamp' ]\n",
    "# tj = t_ddf[cols].merge(t_ddf[cols], how=\"left\", left_on='user_id', right_on='user_id',suffixes=('_c','_p'),npartitions=12000) #\n",
    "# tjf = tj[tj.timestamp_c > tj.timestamp_p]\n",
    "# #tjf.\n",
    "# #df.repartition(npartitions=10) \n",
    "# #tjf = tjf.persist()\n",
    "# print(len(tjf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Index Join, Partition =5.0MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42021035 473\n"
     ]
    }
   ],
   "source": [
    "tu_ddf = t_ddf.set_index('user_id').repartition(partition_size=\"2.5MB\")\n",
    "print(len(tu_ddf),tu_ddf.npartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataframe distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Distribution\n",
    "def cl(df) :\n",
    "    return len(df)\n",
    "\n",
    "def part_dist(df) :\n",
    "    #rv=  df.groupby('user_id').agg({'user_id':'count'})\n",
    "    rv=  df.index.value_counts().to_dict()\n",
    "    rv = {k: v for k, v in sorted(rv.items(), key=lambda item: -1*item[1])}\n",
    "    rv = np.histogram(list(rv.values()),bins=[1,100,500,1000,5000,10000])\n",
    "    #return zip(rv.index, rv.values)\n",
    "    return rv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      ([566, 170, 43, 0, 0], [1, 100, 500, 1000, 500...\n",
       "1      ([591, 167, 40, 0, 0], [1, 100, 500, 1000, 500...\n",
       "2      ([600, 167, 39, 0, 0], [1, 100, 500, 1000, 500...\n",
       "3      ([656, 166, 40, 0, 0], [1, 100, 500, 1000, 500...\n",
       "4      ([570, 156, 44, 0, 0], [1, 100, 500, 1000, 500...\n",
       "                             ...                        \n",
       "468    ([669, 155, 44, 0, 0], [1, 100, 500, 1000, 500...\n",
       "469    ([565, 173, 43, 0, 0], [1, 100, 500, 1000, 500...\n",
       "470    ([561, 164, 47, 0, 0], [1, 100, 500, 1000, 500...\n",
       "471    ([566, 178, 40, 0, 0], [1, 100, 500, 1000, 500...\n",
       "472    ([577, 162, 44, 0, 0], [1, 100, 500, 1000, 500...\n",
       "Length: 473, dtype: object"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.histogram(list(pl[50].values()),bins=[100,500,1000,5000,10000])\n",
    "pl=tu_ddf.map_partitions(part_dist).compute()\n",
    "print(len(pl))\n",
    "pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['content_id', 'answered_correctly','timestamp' ]\n",
    "tju = tu_ddf[cols].merge(tu_ddf[cols], how=\"inner\", left_index=True, right_index=True, suffixes=('_c','_p'),npartitions=1000) #\n",
    "tjfu = tju[tju.timestamp_c > tju.timestamp_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tjfu.visualize(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "tjfu = tjfu.persist()\n",
    "# wait a minute, this triggers a backgroupnd task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7920502621\n"
     ]
    }
   ],
   "source": [
    "#tjfu = tjfu.repartition(partition_size=\"500MB\")\n",
    "print(len(tjfu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Q/A history statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253485474\n"
     ]
    }
   ],
   "source": [
    "tjg = tjfu.groupby(['content_id_c','answered_correctly_c','content_id_p']).agg({'answered_correctly_p':['sum','count']},split_out=20)\n",
    "tjg = tjg.persist()\n",
    "print(len(tjg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">answered_correctly_p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_id_c</th>\n",
       "      <th>answered_correctly_c</th>\n",
       "      <th>content_id_p</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>15</th>\n",
       "      <td>25.0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>43.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>26.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">13522</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>12502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12671</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12918</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12966</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13219</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253485474 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answered_correctly_p      \n",
       "                                                                sum count\n",
       "content_id_c answered_correctly_c content_id_p                           \n",
       "0            0                    15                           25.0    48\n",
       "                                  23                            4.0     8\n",
       "                                  51                           43.0    60\n",
       "                                  63                           10.0    20\n",
       "                                  79                           26.0    32\n",
       "...                                                             ...   ...\n",
       "13522        1                    12502                         0.0     1\n",
       "                                  12671                         0.0     1\n",
       "                                  12918                         1.0     1\n",
       "                                  12966                         1.0     1\n",
       "                                  13219                         1.0     1\n",
       "\n",
       "[253485474 rows x 2 columns]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tjg.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     267804658\n",
       "1     267508201\n",
       "2     267711985\n",
       "3     267643651\n",
       "4     267671539\n",
       "5     267799513\n",
       "6     267681611\n",
       "7     267659981\n",
       "8     267658918\n",
       "9     267812189\n",
       "10    267557627\n",
       "11    267736471\n",
       "12    267571747\n",
       "13    267665806\n",
       "14    267830740\n",
       "15    267710284\n",
       "16    267682501\n",
       "17    267595309\n",
       "18    267727441\n",
       "19    267708730\n",
       "dtype: int64"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tjg.memory_usage_per_partition().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35714285714285715\n",
      "0.7\n",
      "0.45454545454545453\n",
      "0.75\n",
      "0.46875\n"
     ]
    }
   ],
   "source": [
    "# tjg.to_parquet(\"nb_model_lt_1K.pq\")\n",
    "#c= tjg.loc[(0),:].compute()\n",
    "#a = tjg.map_partitions(lambda df : df.iloc[0:1]).compute()\n",
    "idx = (0,0,0)\n",
    "#b = tjg.map_partitions(lambda df : df.index[0]).compute()\n",
    "\n",
    "def getvals(df,idx) :\n",
    "    try :\n",
    "        rv = df.loc[idx]\n",
    "    except :\n",
    "        rv=pd.Series(dtype='int64')\n",
    "        \n",
    "    return rv\n",
    "\n",
    "c = tjg.map_partitions(getvals,idx).compute()\n",
    "\n",
    "def get_prob(idx) :\n",
    "    prob = tjg.map_partitions(getvals,idx).compute()\n",
    "    return prob[0]/prob[1]\n",
    "for i in range(0,5) :\n",
    "    print( get_prob((0,0,i)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.unstack(level=1)\n",
    "b.loc[(0,0)]\n",
    "a.loc[(0,0,0)]\n",
    "\n",
    "a.index = a.index.rename(['a','b','c'])\n",
    "a.loc[(0,0,15)]\n",
    "# Multi-column access\n",
    "type(a.index[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answered_correctly_p  sum      25.0\n",
       "                      count    48.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Data prep] Build NB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct / Total count stored in a List(fornow ..) (num correct , total)\n",
    "\n",
    "class QuestionHistory:\n",
    "    def __init__(self,q_idx) :\n",
    "        self.q_idx = q_idx\n",
    "        self.qh_correct = {}\n",
    "        self.qh_incorrect = {}\n",
    "        self.correct = 0\n",
    "        self.total_q = 0\n",
    "        self.print_threshold = 0\n",
    "    \n",
    "    # Inserrt hirstory into either Answer Correct / Incorrect history\n",
    "    def insert_history(self, answer_correct, q_idx, q_idx_res, debug=False) :\n",
    "        #if(q_idx == self.q_idx) :\n",
    "        #    print(\"Warning: q_idx:{} shouldnt be inserting question result for current question model here\".format(q_idx))\n",
    "        #    return\n",
    "        \n",
    "        if debug : npt(\"Current Question Model : {}  Current question result:{}\".format(self.q_idx,answer_correct))\n",
    "        if debug : npt(\"Adding history for Question:{} with result:{}\".format(q_idx,q_idx_res))\n",
    "\n",
    "        qh_list = self.qh_correct if answer_correct == True else self.qh_incorrect\n",
    "        if(q_idx not in qh_list.keys()) :\n",
    "            qh_list[q_idx] = [q_idx_res,1]  # correct,total cnt\n",
    "        else :\n",
    "            if(q_idx_res) :\n",
    "                qh_list[q_idx][0] += 1\n",
    "            qh_list[q_idx][1] += 1\n",
    "    \n",
    "    def add_result(self, answer_correct) :\n",
    "        if(answer_correct) :\n",
    "            self.correct +=1\n",
    "        self.total_q +=1\n",
    "        \n",
    "    # Utility to return question history prob | answer correct/incorret \n",
    "    # P(QH_i | AC), P(QH_i | AI)\n",
    "    def qh_prob(self, answer_correct, idx, debug=False) :\n",
    "        epsilon = 1e-3\n",
    "        default_prob = 0.5\n",
    "        qh_list = self.qh_correct if answer_correct else self.qh_incorrect\n",
    "        if(idx in qh_list.keys()) :\n",
    "            qh_prob = qh_list[idx][0]/qh_list[idx][1]\n",
    "            if debug : npt(\"question {} given answer_correct={}  correct_cnt={} total={} \".format(idx,answer_correct,qh_list[idx][0],qh_list[idx][1]))\n",
    "        else :\n",
    "            qh_prob = 0.5 # no record yet1\n",
    "        if(qh_prob == 0.0): \n",
    "            qh_prob = epsilon\n",
    "        elif(qh_prob == 1.0):\n",
    "            qh_prob -= epsilon\n",
    "        \n",
    "        if(qh_prob == 0) :\n",
    "            npt(\"error\")\n",
    "        \n",
    "        return qh_prob\n",
    "\n",
    "        \n",
    "    # Average probability of getting this answer correct in general\n",
    "    # if small sample size, then just do 50%\n",
    "    def q_prob(self) :\n",
    "        epsilon = 0.001\n",
    "        rv = 0.5 if self.total_q == 0 else self.correct /  self.total_q\n",
    "        if(rv == 0) :\n",
    "            rv += epsilon\n",
    "        return rv\n",
    "    \n",
    "    def filter_low_freq_questions(self, answer_correct, idx_list,nb_history_minumum=10) :\n",
    "        '''\n",
    "        if not too many questions answered , then just remove otherwise it will screw up calc\n",
    "        '''\n",
    "        qh_list = self.qh_correct if answer_correct else self.qh_incorrect\n",
    "        rv = [q for q in idx_list if q in qh_list.keys()]\n",
    "        rv = [q for q in rv if qh_list[q][1] > nb_history_minumum]\n",
    "        return rv\n",
    "        \n",
    "        \n",
    "    def log_ratio_qh(self, correct_question_idx, incorrect_question_idx, nb_history_minumum=1,debug=False) :\n",
    "        '''\n",
    "        Calculate log probability of question history\n",
    "        return p = p1*...pn / p1*...pn + (1-p1)*....(1-pn) turned to summation.  log p1 + ...\n",
    "        '''\n",
    "        correct_question_idx   = self.filter_low_freq_questions(1,correct_question_idx,nb_history_minumum)\n",
    "        incorrect_question_idx = self.filter_low_freq_questions(1,incorrect_question_idx,nb_history_minumum)\n",
    "        # avoid div by zero\n",
    "        epsilon = 1e-3 if(self.q_prob() == 1.0) else 0.0\n",
    "        log_ac_ai = np.log((self.q_prob()-epsilon )/ (1-(self.q_prob()-epsilon) ))\n",
    "\n",
    "        if debug : npt(\"Question:{} average correct:{} log(p/1-p):{}\".format(self.q_idx,self.q_prob(),log_ac_ai))\n",
    "\n",
    "        log_qh_g_ac = 0\n",
    "        log_qh_g_ai = 0\n",
    "        # handle correct \n",
    "        corr_eqn = \"\\nP(QH|AC) P(AC) =\\n\"\n",
    "        incorr_eqn = \"\\nP(QH|AI) P(AI) =\\n\"\n",
    "        for i, q_list in enumerate([incorrect_question_idx,correct_question_idx]) :\n",
    "            if(i==1) :\n",
    "                if debug : npt(\"Correct Bin for question {}\".format(self.q_idx))\n",
    "            else :\n",
    "                if debug : npt(\"Incorrect Bin for question {}\".format(self.q_idx))\n",
    "            \n",
    "            if debug : npt(\"{:10s} {:10s} {:10s} {:10s} {:10s}\".format(\"question\", \"corr%\", \"inc%\", \"logcorr%\", \"loginc%\"))\n",
    "            for q in q_list :\n",
    "                # self.qh_prob(1, q) # probability QH question coreret given answer correct for current question\n",
    "                # self.qh_prob(0, q) # probability QH question coreret given answer incorrect for current question\n",
    "                if debug : npt(\"{:<10d} {:<10.3f} {:<10.3f} {:<10.3f} {:<10.3f}\".format(q,self.qh_prob(i, q),1-self.qh_prob(i, q),np.log(self.qh_prob(i, q)),np.log(1-self.qh_prob(i, q))))\n",
    "\n",
    "                if(i==1) : # corr\n",
    "                    log_qh_g_ac += np.log(self.qh_prob(1, q))\n",
    "                    if debug :corr_eqn += \"P(Q{}=1|AC) {} * \".format(q, self.qh_prob(1, q))\n",
    "                    log_qh_g_ai += np.log(self.qh_prob(0, q))\n",
    "                    if debug :incorr_eqn += \"P(Q{}=1|AI) {} * \".format(q, self.qh_prob(0, q))\n",
    "                else : # inc\n",
    "                    log_qh_g_ac += np.log(1-self.qh_prob(1, q))\n",
    "                    if debug :corr_eqn += \"P(Q{}=0|AC) {} * \".format(q, 1-self.qh_prob(1, q))\n",
    "                    log_qh_g_ai += np.log(1-self.qh_prob(0, q))\n",
    "                    if debug :incorr_eqn += \"P(Q{}=0|AI) {} * \".format(q, 1-self.qh_prob(0, q))\n",
    "                    \n",
    "        # Debug eqn\n",
    "        if debug :corr_eqn += \"P(AC) {}\".format(self.q_prob())\n",
    "        if debug :incorr_eqn += \"P(AC) {}\".format(1-self.q_prob())\n",
    "\n",
    "        if debug : npt(\"{}\".format(corr_eqn))\n",
    "        if debug : npt(\"{}\".format(incorr_eqn))\n",
    "        # spam classifier equation !\n",
    "        if debug : print(\"log(ac/ai) {}, logsum qh given correct {} logsum qh given incorr {}\".format( log_ac_ai ,log_qh_g_ac , log_qh_g_ai))\n",
    "        log_prob_qh = log_ac_ai + log_qh_g_ac - log_qh_g_ai\n",
    "        return log_prob_qh, sigmoid(log_prob_qh)\n",
    "    \n",
    "    def prob_qh(self, qh_idxs) :\n",
    "        return np.exp(self.log_prob_qh(qh_idxs))\n",
    "    \n",
    "    \n",
    "    def __add__(self,other) :\n",
    "        _qm = QuestionHistory(self.q_idx)\n",
    "        _qm.qh_correct = copy.deepcopy(self.qh_correct)\n",
    "        #print(self.qh_correct)\n",
    "        #print(other.qh_correct)\n",
    "        for k,[c,t] in other.qh_correct.items():\n",
    "            if(k in _qm.qh_correct.keys()) :\n",
    "                _qm.qh_correct[k][0] += c\n",
    "                _qm.qh_correct[k][1] += t\n",
    "            else :\n",
    "                _qm.qh_correct[k] = [c,t]\n",
    "        _qm.qh_incorrect = copy.deepcopy(self.qh_incorrect)\n",
    "        #print(self.qh_incorrect)\n",
    "        #print(other.qh_incorrect)\n",
    "        for k,[c,t] in other.qh_incorrect.items():\n",
    "            if(k in _qm.qh_incorrect.keys()) :\n",
    "                _qm.qh_incorrect[k][0] += c\n",
    "                _qm.qh_incorrect[k][1] += t\n",
    "            else :\n",
    "                _qm.qh_incorrect[k] = [c,t]\n",
    "                #qm.qh_incorrect[k][ = t\n",
    "                \n",
    "        #qm.qh_incorrect = self.qh_incorrect + other.qh_incorrect\n",
    "        _qm.correct = self.correct +other.correct\n",
    "        _qm.total_q = self.total_q +other.total_q\n",
    "        return _qm\n",
    "        \n",
    "    def __repr__(self) :\n",
    "        count_threshold = self.print_threshold\n",
    "        rv = \"Question {} model. Only printing for question history > {}\\n\".format(self.q_idx,count_threshold)\n",
    "        rv += \"Total num correct  :{}\\n\".format(self.correct)\n",
    "        rv += \"Total num questions:{}\\n\".format(self.total_q)\n",
    "        for i,qhl in enumerate([self.qh_incorrect, self.qh_correct]) :\n",
    "            rv += \"incorrect bin\\n\" if i == 0 else \"correct bin\\n\"\n",
    "            for k,cor_tot in qhl.items() :\n",
    "                if(cor_tot[1] > count_threshold):\n",
    "                    rv += \"Question {} Corr,Total {} q_prob {}\\n\".format(k,cor_tot,self.qh_prob(i,k))\n",
    "        return rv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm = QuestionHistory(0)\n",
    "# question 0 model.  if q1 correct, q0 correct\n",
    "#  if q2 correct q0 incorrect\n",
    "\n",
    "\n",
    "# User 1 Q history \n",
    "qm.insert_history(answer_correct=1,q_idx=1,q_idx_res=1)\n",
    "qm.insert_history(answer_correct=1,q_idx=1,q_idx_res=1)\n",
    "qm.insert_history(answer_correct=1,q_idx=1,q_idx_res=1)\n",
    "qm.insert_history(answer_correct=1,q_idx=1,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=1,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=1,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=1,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=1,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=1,q_idx=2,q_idx_res=1)\n",
    "qm.insert_history(answer_correct=1,q_idx=3,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=1,q_idx=4,q_idx_res=1)\n",
    "qm.add_result(1)\n",
    "\n",
    "# User 2 Q history \n",
    "qm.insert_history(answer_correct=0,q_idx=1,q_idx_res=0,debug=True)\n",
    "qm.insert_history(answer_correct=0,q_idx=1,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=1,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=1,q_idx_res=1)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=1)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=1)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=3,q_idx_res=0)\n",
    "qm.insert_history(answer_correct=0,q_idx=4,q_idx_res=1)\n",
    "qm.add_result(0)\n",
    "#\n",
    "qm1 = QuestionHistory(0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=1,q_idx_res=0,debug=True)\n",
    "qm1.insert_history(answer_correct=0,q_idx=1,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=1,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=1,q_idx_res=1)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=1)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=1)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=3,q_idx_res=0)\n",
    "qm1.insert_history(answer_correct=0,q_idx=2224,q_idx_res=1)\n",
    "qm1.insert_history(answer_correct=0,q_idx=14,q_idx_res=1)\n",
    "qm1.add_result(0)\n",
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = qm1 + qm\n",
    "np.sum([qm1,qm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm.filter_low_freq_questions(1,[1,2,99],nb_history_minumum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qm)\n",
    "print(qm.log_ratio_qh([2,4],[1,3],nb_history_minumum=0)) # asking the question relative to correct only ..\n",
    "#history.prob_qh([1,2]) # correct only !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QM_DICT\n",
    "\n",
    "This is a class to hold a dictionary of question models and add them together in a reduction .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QmDict():\n",
    "    def __init__(self,qm_dict):\n",
    "        self.qm_dict = copy.deepcopy(qm_dict)\n",
    "        \n",
    "    def __add__(self,other) :\n",
    "        _qmd = QmDict(self.qm_dict)\n",
    "        for k,v in other.qm_dict.items():\n",
    "            if(k in _qmd.qm_dict.keys()) :\n",
    "                _qmd.qm_dict[k] = v + _qmd.qm_dict[k]  # add question models together\n",
    "            else :\n",
    "                _qmd.qm_dict[k] = v\n",
    "        return _qmd\n",
    "    \n",
    "    def keys(self) :\n",
    "        return self.qm_dict.keys()\n",
    "    \n",
    "    def __getitem__(self,key) :\n",
    "        return self.qm_dict[key]\n",
    "    \n",
    "    def __repr__(self) :\n",
    "        rv = \"QmDict object holding {} models\".format(len(self.qm_dict.keys()))\n",
    "        return rv\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmd1 = QmDict({0:qm,1:qm1})\n",
    "qmd2 = QmDict({1:qm,2:qm1})\n",
    "print(qmd1)\n",
    "qmd1.qm_dict[5]= qm1\n",
    "print(qmd1.keys())\n",
    "print(qmd1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qms = qmd1 + qmd2\n",
    "qms = np.sum([qmd1,qmd2])\n",
    "\n",
    "#qms.qm_dict[0]\n",
    "qms.qm_dict[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserHistory():\n",
    "    '''\n",
    "    Class to track ordered history of answers\n",
    "    '''\n",
    "    def __init__(self,user_id, initial_data=None) :\n",
    "        self.user_id = user_id\n",
    "        self.corr_list = []\n",
    "        self.incr_list = []\n",
    "        if(initial_data is not None) :\n",
    "            self.bulk_add(initial_data)   \n",
    "            \n",
    "    def bulk_add(self, df) :\n",
    "        questions = list(df.content_id.values)\n",
    "        answers = list(df.answered_correctly.values)\n",
    "        corr = [q for q,a in zip(questions,answers) if a==1]\n",
    "        incorr = [q for q,a in zip(questions,answers) if a==0]\n",
    "        self.corr_list = self.corr_list +corr\n",
    "        self.incr_list = self.incr_list +incorr\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def single_add(row) :\n",
    "    #    a=1\n",
    "        \n",
    "    def get_history(self) :\n",
    "        return self.corr_list, self.incr_list\n",
    "    \n",
    "    def __repr__(self) :\n",
    "        return \"UserHistory {} with {} corr_list and {} incr_list entries\\n\".format(self.user_id, len(self.corr_list),len(self.incr_list))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "df = train_df.head(3)\n",
    "display(df)\n",
    "user_test = UserHistory(8,df)\n",
    "print(user_test)\n",
    "\n",
    "user_test.bulk_add(df)\n",
    "user_test.bulk_add(df)\n",
    "\n",
    "user_test.get_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mega Model - 15K NB's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaModel() :\n",
    "    def __init__(self, train_df=None, val_df=None) :\n",
    "        self.nb_models = QmDict({})\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.user_history_dict = {}\n",
    "        self.history_depth = 200 # how far back in Q/A history do we scan ...\n",
    "            \n",
    "    \n",
    "    def get_users(self, question_filter=None) :\n",
    "        df = self.train_df\n",
    "        if(question_filter is not None) :\n",
    "            npt(\"Returning only users that answered content_id:{}\".format(question_filter))\n",
    "            df = df[df.content_id==question_filter]\n",
    "        user_ids = sorted(list(df.user_id.unique()))\n",
    "        print(\"Num unique users {}\".format(len(user_ids)))\n",
    "\n",
    "        return user_ids\n",
    "    \n",
    "    def get_user_transactions_from_training(self, user_idx) :\n",
    "        '''\n",
    "        Not to be confused with my user history dict.  This just fetches a pandas \n",
    "        data frame from training dataframe (for now)\n",
    "        '''\n",
    "        user_transacitons_df = self.train_df[self.train_df[\"user_id\"] == user_idx]\n",
    "        return user_transacitons_df\n",
    "    \n",
    "    # This is expensive .. O(user_hist^2)\n",
    "    def process_user(self, user_idx, debug=True) :\n",
    "        npt(\"user:{}\".format(user_idx))\n",
    "        user_history_df = self.get_user_transactions_from_training(user_idx)\n",
    "        # add to custom user history data structre.  To be used later for inference !\n",
    "        user_history_struct = UserHistory(user_idx,user_history_df)\n",
    "        \n",
    "        nr = len(user_history_df)\n",
    "        if debug : print(\"user hist len {}\".format(nr))        \n",
    "        if debug : display(user_history_df)\n",
    "\n",
    "        # use content_id / answered_correctly ...\n",
    "        # 1st row in user_history has no history.  start with second row to end, and on inner loop add hist up to that row..\n",
    "        qh_idx =  list(user_history_df.content_id)\n",
    "        qh_ans =  list(user_history_df.answered_correctly)\n",
    "        qmd = QmDict({})\n",
    "        for i in range(1,nr) :\n",
    "            curr_q_idx = qh_idx[i]\n",
    "            curr_q_correct =  qh_ans[i]\n",
    "            # qm = self.nb_models[curr_q_idx]\n",
    "            qmd.qm_dict[curr_q_idx] = QuestionHistory(curr_q_idx)\n",
    "            qm = qmd.qm_dict[curr_q_idx]\n",
    "            # log result of current question\n",
    "            qm.add_result(curr_q_correct)\n",
    "            \n",
    "            start = 0 if i < self.history_depth else i-self.history_depth\n",
    "            for j in range(start,i) :\n",
    "                prev_q_idx = qh_idx[j]\n",
    "                prev_q_correct =  qh_ans[j]\n",
    "                qm.insert_history(answer_correct=curr_q_correct,q_idx=prev_q_idx,q_idx_res=prev_q_correct, debug=False)\n",
    "          #print(\"Done adding stats for user:{}, history_len:{}\".format(user_idx,nr))\n",
    "        # Emitting these partials to be reduced later by dask!\n",
    "        # process_user no longer side effects ...\n",
    "        return qmd,user_history_struct\n",
    "\n",
    "    def partition_data(self, df, pct_train=0.85):\n",
    "        num_samples = len(df)\n",
    "        split_point = int(pct_train*num_samples)\n",
    "        self.train_df = df.iloc[0:split_point]\n",
    "        self.val_df   = df.iloc[split_point:num_samples]\n",
    "        npt(\"Added {} train {} val records\".format(len(self.train_df),len(self.val_df)))\n",
    "\n",
    "    \n",
    "    def train(self, num_users=None, user_print_freq=1000,pct_print_freq=5):\n",
    "        user_list = self.get_users()\n",
    "        \n",
    "        prev_pct=-1\n",
    "        job_list = []\n",
    "        \n",
    "        # Create a bunch of dask delayed objects !\n",
    "        for i,user in enumerate(user_list) :\n",
    "            job_list.append(dask.delayed(self.process_user)(user,debug=False))\n",
    "        \n",
    "        npt(\"Processing {} jobs\".format(len(job_list)))\n",
    "        job_list = dask.compute(job_list)\n",
    "        \n",
    "        # job_list is interleaved here ... and has for tuple([[qm,uh],[qm,uh]] , blank)\n",
    "        # To handle qm_dict and user_history separtely, break apart and handle\n",
    "        # Convert [qm,uh],[qm,uh] => [qm,qm],[uh,uh]\n",
    "        # neat zip trick !\n",
    "        [qmd,user_history_list] = list(zip(*job_list[0]))\n",
    "        # print(qmd)\n",
    "        # print(uh)\n",
    "        ##################\n",
    "        # handle qmd /nbmodels\n",
    "        ##################\n",
    "        npt(\"Tree sum for nb_models\")\n",
    "        def add(q1,q2):\n",
    "            return q1+q2\n",
    "        add = dask.delayed(add)\n",
    "        # Tree Sum ! -> https://examples.dask.org/delayed.html\n",
    "        L=qmd\n",
    "        while len(L) > 1:\n",
    "            print(len(L))\n",
    "            new_L = []\n",
    "            for i in range(0, len(L)-1, 2):\n",
    "                lazy = add(L[i], L[i + 1])  # add neighbors\n",
    "                new_L.append(lazy)\n",
    "            L = new_L                       # swap old list for new\n",
    "        #display(L.visualize())\n",
    "        # Compute final tree sum\n",
    "        nbm = dask.compute(L)\n",
    "        # nbm is a tuple([QmDict],blank) to get QmDict need nb[0][0]\n",
    "        self.nb_models = nbm[0][0]\n",
    "        \n",
    "        ##################\n",
    "        # handle userhistory\n",
    "        ##################\n",
    "        npt(\"Adding user history\")\n",
    "        for uh in user_history_list :\n",
    "            self.user_history_dict[uh.user_id] = uh\n",
    "\n",
    "        npt(\"Completed!\")\n",
    "        \n",
    "            #pct = int(100*i/len(user_list))\n",
    "            \n",
    "            #if(i%user_print_freq == 0 ) :\n",
    "            #    print(\"Processed {} users \".format(i))\n",
    "        #\n",
    "            #\n",
    "            #if(pct!=prev_pct and pct%pct_print_freq==0) :\n",
    "            #    print(\"Processed {} %\".format(pct))\n",
    "            #    prev_pct=pct\n",
    "            ##    break\n",
    "        #\n",
    "            #if(num_users is not None and i>num_users) : \n",
    "            #    break\n",
    "#\n",
    "\n",
    "    # Inference hacks fomr now\n",
    "    def inference_row(self,user_id,content_id,nb_history_minumum,debug) :\n",
    "        \n",
    "        user_hist = mm.user_history_dict[user_id].get_history() if user_id in mm.user_history_dict.keys() else [[],[]]\n",
    "        # user_hist -> corr_list, incorr_list\n",
    "        #print(mm.nb_models[content_id])\n",
    "        prediction = [9999999, 0.505050505050505]\n",
    "        if(content_id in mm.nb_models.keys()) :\n",
    "            prediction = mm.nb_models[content_id].log_ratio_qh(user_hist[0],user_hist[1],nb_history_minumum=nb_history_minumum,debug=debug)\n",
    "        else :\n",
    "            npt(\"Warning no model yet avail for content_id {}\".format(content_id))\n",
    "        if debug : npt(\"Prediction = {}\".format(prediction))\n",
    "        prob_correct = prediction[1]\n",
    "        return prob_correct\n",
    "    \n",
    "    def inference_df(self,df,nb_history_minumum=1,debug=False) :\n",
    "        \n",
    "        dfn = df.copy()\n",
    "        tmp = pd.Series([self.inference_row(uid,cid,nb_history_minumum,debug) for uid,cid in zip(df['user_id'],df['content_id']) ], index=df.index)\n",
    "        #npt(tmp, len(tmp),len(df))\n",
    "        dfn.loc[:,\"answered_correctly\"] = tmp\n",
    "        \n",
    "        # return an inference_df ..\n",
    "        if(\"row_id\" not in df.columns) :\n",
    "            npt(\"Adding row_id=user_id for local inferencing\")\n",
    "            dfn[\"row_id\"] = dfn[\"user_id\"]\n",
    "        # This is what the final submittal tool desires - here row_id can just be the index if inferencing on validation set.\n",
    "        return dfn[['row_id','answered_correctly','content_type_id','content_id']]\n",
    "\n",
    " \n",
    "    def rmse(self,df,qa_df) :\n",
    "        a=0\n",
    "    \n",
    "    def histogram_stats(self) :\n",
    "        a=0\n",
    "        # get the number of correct vs incorrect per user / per question!\n",
    "    \n",
    "    def __repr__(self):\n",
    "        rv = \"Model Information\\n\"\n",
    "        # Number Models ->\n",
    "        rv += \"Num of models total         : {}\\n\".format(len(self.nb_models.qm_dict))\n",
    "        rv += \"Num of models updated       : {}\\n\".format(sum([1 for k,v in mm.nb_models.qm_dict.items() if v.total_q > 0]))\n",
    "        rv += \"Num of models updated w/corr: {}\\n\".format(sum([1 for k,v in mm.nb_models.qm_dict.items() if len(v.qh_correct) > 0]))\n",
    "        rv += \"Num of models updated w/incr: {}\\n\".format(sum([1 for k,v in mm.nb_models.qm_dict.items() if len(v.qh_incorrect) > 0]))\n",
    "        \n",
    "        # Number of Users\n",
    "        rv += \"Num of unique users processed: {}\\n\".format(len(self.user_history_dict.keys()))\n",
    "        # Training Size \n",
    "        if(self.train_df is not None) : \n",
    "            rv += \"training size : {}\\n\".format(len(self.train_df))\n",
    "            rv += \"training num uniq questions : {}\\n\".format(len(self.train_df.content_id.unique()))\n",
    "            rv += \"training num uniq users     : {}\\n\".format(len(self.train_df.user_id.unique()))\n",
    "        # val size\n",
    "        if(self.val_df is not None) :\n",
    "            rv += \"valid size : {}\\n\".format(len(self.val_df))\n",
    "            rv += \"training num uniq questions : {}\\n\".format(len(self.val_df.content_id.unique()))\n",
    "            rv += \"training num uniq users     : {}\\n\".format(len(self.val_df.user_id.unique()))\n",
    "            user_intersection = set(self.val_df.user_id.unique()).intersection(self.train_df.user_id.unique())\n",
    "            question_intersection = set(self.val_df.content_id.unique()).intersection(self.train_df.content_id.unique())\n",
    "            rv += \"training/val num intersection users     : {}\\n\".format(len(user_intersection))\n",
    "            rv += \"training/val num intersection questions : {}\\n\".format(len(question_intersection))\n",
    "         # Avg user history\n",
    "        # TBD\n",
    "        return rv\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MegaModel \n",
    "* Todo add dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your data .. use dask [hmm yes]??\n",
    "# cluster = LocalCluster(n_workers=10)\n",
    "# client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# print(client)\n",
    "# cluster.close()\n",
    "# print(cluster)\n",
    "# del cluster\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mm = MegaModel()\n",
    "jl=None\n",
    "with LocalCluster(n_workers=40,\n",
    "    processes=True,\n",
    "    threads_per_worker=1,\n",
    "    ip='tcp://p10a114.pbm.ihost.com:9994',\n",
    "    dashboard_address=':6555'\n",
    ") as cluster, Client(cluster) as client:\n",
    "\n",
    "    print(mm)    \n",
    "    # if i just take the first N rows, the cardinatlity of users is to high for me to test !! \n",
    "    # keeping a smaller set of users helps testing ..\n",
    "    downsample_df = full_df[full_df['user_id']%1900 == 0]\n",
    "    downsample_df = downsample_df[downsample_df['content_id']%1 == 0]\n",
    "    mm.partition_data(downsample_df,pct_train=0.80)\n",
    "    mm.train( num_users=None)\n",
    "    #print(mm)\n",
    "\n",
    "\n",
    "#mm\n",
    "# Processing // Training Algorithm \n",
    "# Foreach user in dset\n",
    "# get history. \n",
    "# from bottom process backwards and add samples to correct / incorrect counters for the question at hand based on history\n",
    "#foreach row in df .. do\n",
    "#add result!\n",
    "#assert(len(user_list)==393656)\n",
    "#user_list = mm.get_users(question_filter=675)\n",
    "#len(user_list)\n",
    "#assert(len(user_list)==16726)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask.visualize(L)\n",
    "# M=dask.compute(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chek size of objects\n",
    "# sys.getsizeof(mm) #56\n",
    "# sys.getsizeof(mm.nb_models.qm_dict)\n",
    "\n",
    "mm.nb_models+mm.nb_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert [qm,uh],[qm,uh] => [qm,qm],[uh,uh]\n",
    "# neat zip trick !\n",
    "#qmd_uh = list(zip(*jl[0]))\n",
    "#qmd_uh[0] # just qmd !\n",
    "#dask.compute(qmd_uh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm.nb_models\n",
    "#mm.user_history_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Examle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_agg = mm.train_df[['user_id','content_id']].groupby(['user_id']).agg([ 'count'])\n",
    "#user_agg.columns = ['count']\n",
    "#multi_q_users = user_agg[user_agg['count']>1]\n",
    "#multi_q_users = multi_q_users.index.values\n",
    "#mm.train_df[mm.train_df['user_id'] in multi_q_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm.nb_models[4301]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "inf_df = mm.val_df.iloc[0:300,:]\n",
    "ans_df = mm.inference_df(inf_df,nb_history_minumum=5,debug=False)\n",
    "display(mm.val_df.head(2))\n",
    "ans_df[\"answered_correctly_orig\"] = inf_df[\"answered_correctly\"] \n",
    "# add simple question averages column\n",
    "\n",
    "display(question_averages_df.head())\n",
    "question_averages_df.columns = [\"total_correct\",\"total_questions\",\"avg_correct\"]\n",
    "ans_df = ans_df.merge(question_averages_df,how=\"inner\",left_on=\"content_id\",right_on=\"content_id\")\n",
    "\n",
    "ans_df[\"nb_sqerr\"] = np.power(ans_df[\"answered_correctly\"]-ans_df[\"answered_correctly_orig\"],2)\n",
    "ans_df[\"avg_sqerr\"] = np.power(ans_df[\"avg_correct\"]-ans_df[\"answered_correctly_orig\"],2)\n",
    "\n",
    "\n",
    "display(ans_df)\n",
    "print(\"nb sse \", np.sum(ans_df.nb_sqerr.values))\n",
    "print(\"avg sse \", np.sum(ans_df.avg_sqerr.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.nb_models[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "pickle.dump(obj=mm, file=open(\"mm_147900.nbm\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l\n",
    "sys.getsizeof(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User number\")\n",
    "idx=2\n",
    "print(user_list[idx])\n",
    "print(\"Correct // Incorrect history\")\n",
    "print(mm.user_history_dict[user_list[idx]].get_history())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Simple inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get userid / content_id ...\n",
    "row = val_df.iloc[0]\n",
    "row.user_id\n",
    "\n",
    "user_hist = train_df[train_df.user_id == row.user_id]\n",
    "\n",
    "prediction = mm.nb_models[row.content_id].log_ratio_qh([],[],tot_cutoff=1)\n",
    "#def log_ratio_qh(self, correct_question_idx, incorrect_question_idx, tot_cutoff=10,debug=True) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm.nb_models[675].correct)\n",
    "print(mm.nb_models[675].total_q)\n",
    "\n",
    "print(mm.nb_models[675].log_ratio_qh([1291],[731],tot_cutoff=0)) # asking the question relative to correct only ..\n",
    "#print(mm.nb_models[675].qh_correct[731])\n",
    "#print(mm.nb_models[675].qh_incorrect[731])\n",
    "print(mm.nb_models[675])\n",
    "\n",
    "#Key 4078 Value [3, 8] q_prob 0.375\n",
    "#Key 1291 Value [11, 14] q_prob 0.7857142857142857\n",
    "#Key 1131 Value [7, 11] q_prob 0.6363636363636364\n",
    "#Key 1145 Value [11, 11] q_prob 1.0\n",
    "#Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm.nb_models[675])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=train_df.iloc[0]\n",
    "print(row)\n",
    "qm = mm.nb_models[row.content_id]\n",
    "qm.q_idx = row.content_id\n",
    "qm.insert_history(answer_correct=row.answered_correctly,q_idx=4,q_idx_res=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.75*0.75 / (0.75*0.75 + 0.25*0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019728,
     "end_time": "2020-11-10T14:31:04.711421",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.691693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Save out to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025817,
     "end_time": "2020-11-10T14:31:04.755335",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.729518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question_averages_df.to_csv('qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026021,
     "end_time": "2020-11-10T14:31:04.799659",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.773638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018166,
     "end_time": "2020-11-10T14:31:04.836043",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.817877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025846,
     "end_time": "2020-11-10T14:31:04.880173",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.854327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## # You can only iterate through a result from `env.iter_test()` once\n",
    "# so be careful not to lose it once you start iterating.\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025992,
     "end_time": "2020-11-10T14:31:04.924882",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.898890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (test_df, sample_prediction_df) = next(iter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026094,
     "end_time": "2020-11-10T14:31:04.970639",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.944545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026083,
     "end_time": "2020-11-10T14:31:05.015746",
     "exception": false,
     "start_time": "2020-11-10T14:31:04.989663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.030966,
     "end_time": "2020-11-10T14:31:05.065922",
     "exception": false,
     "start_time": "2020-11-10T14:31:05.034956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prediction(test_df, question_averages_df) :\n",
    "    # Join Tables ...\n",
    "    prediction_df = test_df[['row_id','content_id','content_type_id']].merge(question_averages_df,left_on='content_id',right_on='content_id',how='left')\n",
    "    # import numpy as np\n",
    "    # prediction_df['random'] = np.random.rand(len(prediction_df))\n",
    "    prediction_df = prediction_df.rename(columns={\"pct_correct\": \"answered_correctly\"})\n",
    "    print(\"testdf len : {}\".format(len(test_df)))\n",
    "    print(\"DF len : {}\".format(len(prediction_df)))\n",
    "    return prediction_df[['row_id','answered_correctly','content_type_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026904,
     "end_time": "2020-11-10T14:31:05.112379",
     "exception": false,
     "start_time": "2020-11-10T14:31:05.085475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Function\n",
    "# prediction_df = make_prediction(test_df,question_averages_df)\n",
    "# env.predict(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027692,
     "end_time": "2020-11-10T14:31:05.160255",
     "exception": false,
     "start_time": "2020-11-10T14:31:05.132563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.529514,
     "end_time": "2020-11-10T14:31:05.709864",
     "exception": false,
     "start_time": "2020-11-10T14:31:05.180350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    test_df = test_df.loc[test_df.content_type_id == 0].reset_index(drop=True)\n",
    "    prediction_df = make_prediction(test_df,question_averages_df)\n",
    "    env.predict(prediction_df[['row_id', 'answered_correctly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041616,
     "end_time": "2020-11-10T14:31:05.771859",
     "exception": false,
     "start_time": "2020-11-10T14:31:05.730243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.060092,
     "end_time": "2020-11-10T14:31:05.853616",
     "exception": false,
     "start_time": "2020-11-10T14:31:05.793524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.022253,
     "end_time": "2020-11-10T14:31:05.898995",
     "exception": false,
     "start_time": "2020-11-10T14:31:05.876742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:oce-pytorch-1.6]",
   "language": "python",
   "name": "conda-env-oce-pytorch-1.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "papermill": {
   "duration": 74.951096,
   "end_time": "2020-11-10T14:31:06.230702",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-10T14:29:51.279606",
   "version": "2.1.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
